{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d325b3-9d11-4cb3-939c-7df5ee509333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'player', 'loves', 'to', 'run', 'and', 'play'],\n",
       " ['running', 'in', 'the', 'sun', 'keeps', 'you', 'healthy'],\n",
       " ['playing', 'outside', 'makes', 'a', 'boy', 'healthy', 'and', 'happy']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "corpus = [\n",
    "    \"The player. loves to run, and play \",\n",
    "    \"Running in the sun keeps you healthy\",\n",
    "    \"Playing outside makes a boy , healthy and happy\"\n",
    "]\n",
    "def tokenize(corpus):\n",
    "    table = str.maketrans('', '', string.punctuation) \n",
    "    docs=[doc.lower().translate(table).split() for doc in corpus]\n",
    "    return (docs)\n",
    "    \n",
    "docs=tokenize(corpus)\n",
    "\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ba69cf-3bdb-400c-9318-cfed1a309c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['player', 'loves', 'run', 'play'],\n",
       " ['running', 'sun', 'keeps', 'healthy'],\n",
       " ['playing', 'outside', 'makes', 'boy', 'healthy', 'happy']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_wordss(docs):\n",
    "    clean_list=[]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for d in docs :\n",
    "        sub_list=[]\n",
    "        for t in d:\n",
    "            if t not in stop_words :\n",
    "                sub_list.append(t)\n",
    "        clean_list.append(sub_list)\n",
    "    return(clean_list)\n",
    "    \n",
    "\n",
    "stop_wordss(tokenize(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c980737a-4f7e-4740-89bb-463526c85e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['player', 'love', 'run', 'play'], ['running', 'sun', 'keep', 'healthy'], ['playing', 'outside', 'make', 'boy', 'healthy', 'happy']]\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "lemmas=[]\n",
    "\n",
    "for d in stop_wordss(tokenize(corpus)) :\n",
    "    sub_list=[]\n",
    "    for t in d:\n",
    "        sub_list.append(Word(t).lemmatize())\n",
    "    lemmas.append(sub_list)\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0050f9-0611-45a2-9942-a801fbc14b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['player', 'love', 'run', 'play'],\n",
       " ['run', 'sun', 'keep', 'healthi'],\n",
       " ['play', 'outsid', 'make', 'boy', 'healthi', 'happi']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "def stemmingg(clean_list):\n",
    "    stems=[]\n",
    "    for d in clean_list :\n",
    "        sub_list=[]\n",
    "        for t in d:\n",
    "            sub_list.append(stemmer.stem(t))\n",
    "        stems.append(sub_list)\n",
    "    return(stems)\n",
    "    \n",
    "stemmingg(stop_wordss(tokenize(corpus)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87fc271-fdf7-4a16-b641-e1a10ac8b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'player': 0.25, 'love': 0.25, 'run': 0.25, 'play': 0.25}, {'run': 0.25, 'sun': 0.25, 'keep': 0.25, 'healthi': 0.25}, {'play': 0.16666666666666666, 'outsid': 0.16666666666666666, 'make': 0.16666666666666666, 'boy': 0.16666666666666666, 'healthi': 0.16666666666666666, 'happi': 0.16666666666666666}]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "stems=stemmingg(stop_wordss(tokenize(corpus)))\n",
    "\n",
    "tf_all = []  \n",
    "for d in stems:\n",
    "    total_terms = len(d)\n",
    "    counts = Counter(d)\n",
    "    tf = {t: counts[t] / total_terms for t in counts} \n",
    "    tf_all.append(tf)\n",
    "print(tf_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f95558a-33f6-43df-9d95-27d5044cfd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'player': 0.47712125471966244, 'run': 0.17609125905568124, 'play': 0.17609125905568124, 'love': 0.47712125471966244, 'make': 0.47712125471966244, 'boy': 0.47712125471966244, 'keep': 0.47712125471966244, 'sun': 0.47712125471966244, 'happi': 0.47712125471966244, 'healthi': 0.17609125905568124, 'outsid': 0.47712125471966244}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "D=len(stemmingg(stop_wordss(tokenize(corpus))))\n",
    "print (D)\n",
    "all_terms = set([word for doc in stems for word in doc]) \n",
    "doc_freq = {} \n",
    "for term in all_terms:\n",
    "    doc_freq[term] = sum(1 for doc in stems if term in doc)\n",
    "\n",
    "idf = {term: math.log10(D / doc_freq[term]) for term in doc_freq}\n",
    "print (idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2fb9b7-2798-497c-bade-04db343485b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'player': 0.11928031367991561, 'run': 0.04402281476392031, 'play': 0.04402281476392031, 'love': 0.11928031367991561, 'make': 0.0, 'boy': 0.0, 'keep': 0.0, 'sun': 0.0, 'happi': 0.0, 'healthi': 0.0, 'outsid': 0.0}, {'player': 0.0, 'run': 0.04402281476392031, 'play': 0.0, 'love': 0.0, 'make': 0.0, 'boy': 0.0, 'keep': 0.11928031367991561, 'sun': 0.11928031367991561, 'happi': 0.0, 'healthi': 0.04402281476392031, 'outsid': 0.0}, {'player': 0.0, 'run': 0.0, 'play': 0.029348543175946873, 'love': 0.0, 'make': 0.07952020911994373, 'boy': 0.07952020911994373, 'keep': 0.0, 'sun': 0.0, 'happi': 0.07952020911994373, 'healthi': 0.029348543175946873, 'outsid': 0.07952020911994373}]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = []\n",
    "for d in tf_all:               \n",
    "    row = {}\n",
    "    for t in all_terms:       \n",
    "        row[t] = d.get(t, 0) * idf[t]\n",
    "    tf_idf.append(row)\n",
    "print (tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ce73285-3a75-46c1-89db-3a672e0a32f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      player   run  play  love  make   boy  keep   sun  happi  healthi  outsid\n",
      "Doc1    0.12  0.04  0.04  0.12  0.00  0.00  0.00  0.00   0.00     0.00    0.00\n",
      "Doc2    0.00  0.04  0.00  0.00  0.00  0.00  0.12  0.12   0.00     0.04    0.00\n",
      "Doc3    0.00  0.00  0.03  0.00  0.08  0.08  0.00  0.00   0.08     0.03    0.08\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tf_idf)\n",
    "df.index = [f\"Doc{i+1}\" for i in range(len(tf_idf))]\n",
    "\n",
    "print(df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a2fba4-73fa-4b60-9047-9bc93cbf5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"tf_idf_matrix.xlsx\", index=True, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc9e054b-59c6-48cb-b47f-37bd4d66150b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ACER\\\\search engine'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341c555a-73bf-4b44-a4b6-cfb6506e11aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\ACER\\\\search engine', 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The player loves to run and play \",\n",
    "    \"Running in the sun keeps you healthy\",\n",
    "    \"Playing outside makes a boy healthy and happy\"\n",
    "]\n",
    "\n",
    "\n",
    "def indexation(corpus,x):\n",
    "    stems=stemmingg(stop_wordss(tokenize(corpus)))\n",
    "    tf_all = []  \n",
    "    for d in stems:\n",
    "        total_terms = len(d)\n",
    "        counts = Counter(d)\n",
    "        tf = {t: counts[t] / total_terms for t in counts} \n",
    "        tf_all.append(tf)\n",
    "    D=len(stemmingg(stop_wordss(tokenize(corpus))))\n",
    "    all_terms = set([word for doc in stems for word in doc]) \n",
    "    doc_freq = {} \n",
    "    for term in all_terms:\n",
    "        doc_freq[term] = sum(1 for doc in stems if term in doc)\n",
    "    idf = {term: math.log10(D / doc_freq[term]) for term in doc_freq}\n",
    "    tf_idf = []\n",
    "    for d in tf_all:               \n",
    "        row = {}\n",
    "        for t in all_terms:       \n",
    "            row[t] = d.get(t, 0) * idf[t]\n",
    "        tf_idf.append(row)\n",
    "    df = pd.DataFrame(tf_idf)\n",
    "    \n",
    "    df.index = [f\"Doc{i+1}\" for i in range(len(tf_idf))]\n",
    "    df.to_excel(x, index=True, engine='openpyxl')\n",
    "    return (os.getcwd(),len(df.columns))\n",
    "    \n",
    "indexation(corpus,\"tf_idf.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8253d442-fda6-4c36-bda6-4282ed07b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_cranfield_bodies(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "    raw_docs = re.split(r\"\\.I\\s+\\d+\", text)\n",
    "    raw_docs = [doc.strip() for doc in raw_docs if doc.strip()]\n",
    "\n",
    "    corpus = []\n",
    "    for raw in raw_docs:\n",
    "        match = re.search(r\"\\.W\\s*(.*)\", raw, re.S)\n",
    "        if match:\n",
    "            body = re.sub(r\"\\s+\", \" \", match.group(1).strip())\n",
    "            corpus.append(body)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "cor = read_cranfield_bodies(r\"C:\\Users\\ACER\\search engine\\cran.all.1400\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee4d65-ee87-4365-8038-8cdad940577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"tf.xlsx\"\n",
    "indexation(cor,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacc8e7-732c-495f-8521-32047c00ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = read_cranfield_bodies(r\"trec.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc1016-94f1-4332-ba1e-a0b87a082eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexation(co,\"res.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f3a1e4-bbf3-4e25-9786-2fa5d1bcc1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"langage java informatique php java langage php java \",\n",
    "    \"langage java php langage \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0215d5f-80ff-40a4-a661-6906623215f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'langag': 0.25, 'java': 0.375, 'informatiqu': 0.125, 'php': 0.25},\n",
       " {'langag': 0.5, 'java': 0.25, 'php': 0.25}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def indexation(req):\n",
    "    stems=stemmingg(stop_wordss(tokenize(req)))\n",
    "    tf_all = []  \n",
    "    for d in stems:\n",
    "        total_terms = len(d)\n",
    "        counts = Counter(d)\n",
    "        tf = {t: counts[t] / total_terms for t in counts} \n",
    "        tf_all.append(tf)\n",
    "    \n",
    "    return (tf_all)\n",
    "CORPUS=indexation(corpus)    \n",
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4624f47-f6c1-4426-ab25-5c080066c72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_term(term,docnum):\n",
    "    for t in CORPUS[docnum]:\n",
    "        if (term==t):\n",
    "            return (CORPUS[docnum][t])\n",
    "get_term('langag',0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81f75c1d-0216-4b9e-b889-2b8026cf3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25]\n"
     ]
    }
   ],
   "source": [
    "req=\"langag AND java OR php\"\n",
    "def similrite(req,d):\n",
    "    list=req.split()\n",
    "    i = 0\n",
    "    while i < len(list):\n",
    "        if i - 1 >= 0 and i + 1 < len(list):\n",
    "            if (list[i]==\"AND\"):\n",
    "                if (type(list[i-1])==str):\n",
    "                    v1=get_term(list[i-1],d)\n",
    "                else:\n",
    "                    v1=list[i-1]\n",
    "                if (type(list[i+1])==str):\n",
    "                    v2=get_term(list[i+1],d)\n",
    "                else:\n",
    "                    v2=list[i+1]\n",
    "                list[i]=min(v1,v2)\n",
    "                del list[i-1]\n",
    "                del list[i]\n",
    "                i =i- 1\n",
    "            else:\n",
    "                i =i+ 1\n",
    "        else:\n",
    "            i =i+ 1 \n",
    "\n",
    "    i = 0\n",
    "    while i < len(list):\n",
    "        if i - 1 >= 0 and i + 1 < len(list):\n",
    "            if (list[i]==\"OR\"):\n",
    "                if (type(list[i-1])==str):\n",
    "                    v1=get_term(list[i-1],d)\n",
    "                else:\n",
    "                    v1=list[i-1]\n",
    "                if (type(list[i+1])==str):\n",
    "                    v2=get_term(list[i+1],d)\n",
    "                else:\n",
    "                    v2=list[i+1]\n",
    "                list[i]=max(v1,v2)\n",
    "                del list[i-1]\n",
    "                del list[i]\n",
    "                i =i- 1\n",
    "            else:\n",
    "                i =i+ 1\n",
    "        else:\n",
    "            i =i+ 1 \n",
    "    return list\n",
    "  \n",
    "print(similrite(req,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebf329-f8b3-4b3c-a023-c9c9f417bef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
